{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN - ISMIR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==1.15.2"
      ],
      "metadata": {
        "id": "ZKVDDQSwOotj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e639dc1b-76c7-49ad-a211-dbb9c4353dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 32 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.47.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=46635652ace6aa0069ad2b403fe876183da9390652fe7c1965a94bb4c50a0942\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard<2.9,>=2.8, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires tensorflow-estimator<2.9,>=2.8, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dbusbridge/gcn_tutorial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AhZePaLQQYg",
        "outputId": "56d8af01-db2e-4783-888b-89d30ff29325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gcn_tutorial'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Total 69 (delta 0), reused 0 (delta 0), pack-reused 69\u001b[K\n",
            "Unpacking objects: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc896PF72ZxG"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse\n",
        "import tensorflow.compat.v1 as tf\n",
        "import gcn_tutorial.layers.graph as lg\n",
        "import gcn_tutorial.utils.sparse as us\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_label(G, emotion_label, df):\n",
        "    for node in G.nodes():\n",
        "        if ':music' in node:            \n",
        "            label = df[df.musicId == int(node.replace(':music', ''))][emotion_label].to_list()[0]\n",
        "            G.nodes[node]['label'] = label            \n",
        "    return G\n",
        "\n",
        "\n",
        "def masked_softmax_cross_entropy(preds, labels, mask):\n",
        "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):\n",
        "    \"\"\"Accuracy with masking.\"\"\"\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)\n"
      ],
      "metadata": {
        "id": "UbtTp3Ux2y_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_gcn(G, run):   \n",
        "    run += 1\n",
        "    network = str(run)\n",
        "    node_list = []\n",
        "    for node in G.nodes():\n",
        "        node_list.append(node)\n",
        "\n",
        "    label_codes = {}\n",
        "    for node in node_list:\n",
        "        if 'label' in G.nodes[node]:\n",
        "            label = G.nodes[node]['label']\n",
        "            if label not in label_codes: label_codes[label] = len(label_codes)\n",
        "            G.nodes[node]['membership'] = label_codes[label]\n",
        "        else:\n",
        "            G.nodes[node]['membership'] = -1\n",
        "\n",
        "    # adj = nx.adj_matrix(G,nodelist=node_list)\n",
        "    adj = nx.adjacency_matrix(G, nodelist=node_list)\n",
        "\n",
        "    # Get important parameters of adjacency matrix\n",
        "    n_nodes = adj.shape[0]\n",
        "\n",
        "    # GCN preprocessing\n",
        "    adj_tilde = adj + np.identity(n=adj.shape[0])\n",
        "    d_tilde_diag = np.squeeze(np.sum(np.array(adj_tilde), axis=1))\n",
        "    d_tilde_inv_sqrt_diag = np.power(d_tilde_diag, -1 / 2)\n",
        "    d_tilde_inv_sqrt = np.diag(d_tilde_inv_sqrt_diag)\n",
        "    adj_norm = np.dot(np.dot(d_tilde_inv_sqrt, adj_tilde), d_tilde_inv_sqrt)\n",
        "    adj_norm_tuple = us.sparse_to_tuple(scipy.sparse.coo_matrix(adj_norm))\n",
        "\n",
        "    \n",
        "    # get true labels\n",
        "    y_true = []\n",
        "    y_true_index = []\n",
        "    cnt = 0\n",
        "    for node in node_list:\n",
        "        if \"split\" in G.nodes[node] and 'test' == G.nodes[node]['split']:\n",
        "            y_true.append(label_codes[G.nodes[node]['label']])\n",
        "            y_true_index.append(cnt)\n",
        "        cnt += 1\n",
        "\n",
        "    # Features from two modalities\n",
        "    L_X = []\n",
        "    for node in node_list:\n",
        "        v1 = list(G.nodes[node]['f_acoustic'])\n",
        "        v2 = list(G.nodes[node]['f_bert'])\n",
        "        v_final = v1 + v2\n",
        "        L_X.append(v_final)\n",
        "    feat_x = np.array(L_X)\n",
        "\n",
        "    feat_x_tuple = us.sparse_to_tuple(scipy.sparse.coo_matrix(feat_x))\n",
        "\n",
        "    # Preparing train data\n",
        "    memberships = [m for m in nx.get_node_attributes(G, 'membership').values()]\n",
        "    nb_classes = len(set(memberships))\n",
        "    targets = np.array([memberships], dtype=np.int32).reshape(-1)\n",
        "    one_hot_targets = np.eye(nb_classes)[targets]\n",
        "\n",
        "    labels_to_keep = []\n",
        "\n",
        "    counter = 0\n",
        "    for node in node_list:\n",
        "        if 'label' in G.nodes[node]:\n",
        "            labels_to_keep.append(counter)\n",
        "        counter += 1\n",
        "\n",
        "    y_train = np.zeros(shape=one_hot_targets.shape,\n",
        "                       dtype=np.float32)\n",
        "\n",
        "    train_mask = np.zeros(shape=(n_nodes,), dtype=np.bool)\n",
        "\n",
        "    for l in labels_to_keep:\n",
        "        y_train[l, :] = one_hot_targets[l, :]\n",
        "        train_mask[l] = True\n",
        "\n",
        "    # TensorFlow placeholders\n",
        "    ph = {\n",
        "        'adj_norm': tf.sparse_placeholder(tf.float32, name=\"adj_mat\"),\n",
        "        'x': tf.sparse_placeholder(tf.float32, name=\"x\"),\n",
        "        'labels': tf.placeholder(tf.float32, shape=(n_nodes, nb_classes)),\n",
        "        'mask': tf.placeholder(tf.int32)}\n",
        "\n",
        "    l_sizes = [512, 256, 128, nb_classes]\n",
        "    print(nb_classes)  # , set(memberships))\n",
        "\n",
        "    o_fc1 = lg.GraphConvLayer(\n",
        "        input_dim=feat_x.shape[-1],\n",
        "        output_dim=l_sizes[0],\n",
        "        name='fc1_' + network,\n",
        "        activation=tf.nn.tanh)(adj_norm=ph['adj_norm'], x=ph['x'], sparse=True)\n",
        "\n",
        "    o_fc2 = lg.GraphConvLayer(\n",
        "        input_dim=l_sizes[0],\n",
        "        output_dim=l_sizes[1],\n",
        "        name='fc2_' + network,\n",
        "        activation=tf.nn.tanh)(adj_norm=ph['adj_norm'], x=o_fc1)\n",
        "\n",
        "    o_fc3 = lg.GraphConvLayer(\n",
        "        input_dim=l_sizes[1],\n",
        "        output_dim=l_sizes[2],\n",
        "        name='fc3_' + network,\n",
        "        activation=tf.nn.tanh)(adj_norm=ph['adj_norm'], x=o_fc2)\n",
        "\n",
        "    o_fc4 = lg.GraphConvLayer(\n",
        "        input_dim=l_sizes[2],\n",
        "        output_dim=l_sizes[3],\n",
        "        name='fc4_' + network,\n",
        "        activation=tf.identity)(adj_norm=ph['adj_norm'], x=o_fc3)\n",
        "\n",
        "    with tf.name_scope('optimizer'):\n",
        "        loss = masked_softmax_cross_entropy(preds=o_fc4, labels=ph['labels'], mask=ph['mask'])\n",
        "        accuracy = masked_accuracy(preds=o_fc4, labels=ph['labels'], mask=ph['mask'])\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "        opt_op = optimizer.minimize(loss)\n",
        "\n",
        "    feed_dict_train = {ph['adj_norm']: adj_norm_tuple,\n",
        "                       ph['x']: feat_x_tuple,\n",
        "                       ph['labels']: y_train,\n",
        "                       ph['mask']: train_mask}\n",
        "   \n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    epochs = 100 #7000\n",
        "    \n",
        "    outputs = {}\n",
        "    # Train model\n",
        "    min_train_acc = 0    \n",
        "    for epoch in tqdm(range(epochs), total=epochs):\n",
        "        _, train_loss, train_acc = sess.run(\n",
        "            (opt_op, loss, accuracy), feed_dict=feed_dict_train)\n",
        "        feed_dict_output = {ph['adj_norm']: adj_norm_tuple, ph['x']: feat_x_tuple}\n",
        "\n",
        "        # print(train_loss,train_acc)        \n",
        "        if train_acc >= min_train_acc:\n",
        "            min_train_acc = train_acc\n",
        "            embeddings = sess.run(o_fc3, feed_dict=feed_dict_output)\n",
        "            preds = sess.run(o_fc4, feed_dict=feed_dict_output)    \n",
        "    y_pred = []\n",
        "    for i in y_true_index:\n",
        "        y_pred.append(preds[i].argmax())\n",
        "\n",
        "    return y_true, y_pred, embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "cjjiQvRg20d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':    \n",
        "    df = pd.read_csv(f\"dataset_base.tsv\", sep='\\t')\n",
        "    run = 1\n",
        "\n",
        "    emotion_label = \"arousal\"\n",
        "    grafo = \"graph.nx\"\n",
        "    G = nx.read_gpickle(f'{grafo}')    \n",
        "    \n",
        "    G = set_label(G=G, emotion_label=emotion_label, df=df)\n",
        "    y_true, y_pred, embeddings = build_gcn(G=G, run=run)\n",
        "    "
      ],
      "metadata": {
        "id": "8qBV-KuW3I02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0g0WgG1VzdA",
        "outputId": "ffda6592-b9e6-4ffc-97a0-424aa2b8b8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1, 0, 1, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 1] [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    }
  ]
}